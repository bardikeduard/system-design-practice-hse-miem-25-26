# HW 1 Solution

## Этап №1: Думаем и проектируем
### Метрики для отслеживания
Для понимания состояния системы нам необходимы следующие метрики:

1. технические метрики для HTTP запросов:
   - RPS
   - errors: количество 5XX и 4XX ответов
   - latency (у меня на работе используются p50, p75, p95, p99)

2. технические метрики для ресурсов (CPU, Memory, Disk, Network):
   - утилизация CPU/RAM контейнеров (backend, db)

3. технические метрики БД:
   - количество активных соединений
   - длительность запросов к БД
   - количество транзакций в секунду

4. бизнес метрики:
   - количество созданных заказов в минуту/час/день/... или их рейт
   - количество зарегистрировавшихся пользователей в минуту/час/день/... или их рейт
   - latency размещения заказов и регистрации пользователей (можно тоже по квантилям): метрика про пользовательский опыт, никто не хочет долго ждать

Технические метрики позволяют мониторить утилизацию ресурсов. Напрямую они могут не влиять на качество работы приложения, но какой-то резкий рост/падение может говорить о том, что где-то система стала работать хуже.
Бизнес-метрики отражают показатели того, ради чего система и создавалась - как хорошо мы зарабатываем, сколько пользователей привлекаем, и т.п.
В нашей системе уже в целом есть все перечисленные метрики. Метрики HTTP запросов и бизнес метрики по сути дублируют друг друга (количество 200 на ручку создания заказа как техническая метрика из п.1, так и бизнес метрика из п.4).
Метрики ресурсов извлекаем с помощью node exporter, метрики БД - postgres_exporter.
Добавил дашборд со статусами и латенси POST-запросов на ручки /api/orders и /api/users - по сути это наши бизнес метрики.

## Этап №2: Стреляем

Я подготовил три сценария нагрузочного тестирования в /hw1/scripts/:

1. Сценарий "Шторм" (storm.js):
   - Резкий скачок до 1000 пользователей за 10 секунд
   - Цель: Проверить устойчивость системы к DDoS-подобным всплескам и скорость восстановления

2. Сценарий "Волна" (wave.js):
   - Плавное нарастание до 500 пользователей за 2 минуты
   - Цель: найти точку насыщения (saturation point), когда латенси начинает деградировать

3. Сценарий "Модификация" (nginx_only.js):
   - Изменение: все запросы идут только через nginx (порт 8080). В оригинальном скрипте POST запросы шли напрямую в бэкенд (8081)
   - Добавление: добавлен запрос `GET /api/users/{id}` для эмуляции более реалистичного профиля чтения.
   - Цель: проверить работу системы в конфигурации, приближенной к продакшену (все через прокси), и нагрузить чтение по ID

## Этап №3: Анализируем
### Инсайты:

1. Неоптимальный nginx config:
   - В docker-compose.yaml подняты два инстанса бэкенда (backend и backend_2), но в конфиге nginx прописан только один `proxy_pass http://backend:8081/api/;`. backend_2 простаивает, нагрузка не балансируется, используем только половину вычислительной мощности бэкенда. В целом для нашего теста это не очень критично, так как первый бэкенд использует весь доступный ЦПУ на моём ноутбуке, но в реальности нужна балансировка и масштабирование.

2. Сценарий "Шторм":
   - При резком скачке пользователей растет задержка и появляются ошибки 502/504. Скорее всего из-за того, что пул соединений к postgresql (max_connections=200) заканчивается

3. Сценарий "Волна":
   - До определенного момента система будет отвечать быстро, а затем начнется линейный рост времени ответа из-за конкуренции за ресурсы БД.

## Этап №4: Решения
### Предложения по оптимизации:

1. Кэширование:
   - Запросы `GET /api/orders` и `GET /api/users/{id}` можно кэшировать (redis/memcached или просто in-memory внутри приложения)

2. Connection Pooling:
   - Настроить пул соединений на бэкенде в соответствии с настройками количества соединений у postgresql

3. Индексы:
   - Добавить индексы в базе данных. Будет полезно если будет иметься какой-то поиск

4. Масштабирование:
   - Внедрить master-slave схему репликации БД. Позволит иметь 2 независимых пула соединений. Запросы на размещение заказа не будут тормозить GET запросы

5. Observability:
   - Скорее эксплуатационное замечание - добавить логирование и трейсинг. Например, у меня на работе с какой-то маленькой вероятностью к параметрам запроса добавляется флаг о необходимости записи трейса и лога. В случае если есть какие-то проблемы, которые не видно на метриках, это позволяет сразу посмотреть уже готовые логи с пользовательских запросов.
